---
title: "Time Series Stationarity"
author: "Alberto Almuiña"
date: '2021-02-25T21:13:14-05:00'
description: An introduction to stationarity in time series and its importance.
slug: stationarity
tags:
- stationarity
- dickey-fuller
- stochastic process
categories: 
- Time Series
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="what-is-a-stochastic-process" class="section level2">
<h2>What is a stochastic process?</h2>
<p>To understand the concept of stationarity, we must first introduce the concept of stochastic process. Suppose that every day I go to my laboratory to repeat exactly the same experiment, under the same conditions (same temperature, humidity, etc.), in which I measure a function of a variable, for example energy as a function of time. What would you expect to see? What nature tells me is that while the results are very similar to each other, they are not exactly the same. But then, which one do I listen to? Are my results worth nothing? Precisely the theory in charge of giving us an answer and helping us to extract useful information from our experiment will be the stochastic processes.</p>
<p>For example, suppose we take the output of two of our experiments (for simplicity):</p>
<pre class="r"><code>library(zeallot)

set.seed(129)

Z &lt;- stats::rnorm(255, 0, 1) 

c(u, u2, sd, sd2, s, a) %&lt;-% c(0.3, 0.28, 0.2, 0.5, 100, 2)

t &lt;- 1:256

energy &lt;- c(s)        
energy2 &lt;- c(s)

for(i in Z) {
  
    S = s + s*(u/255 + sd/sqrt(255)*i)
    S1 = s + s*(u2/255 + sd2/sqrt(255)*i)
    energy[a] &lt;- S 
    energy2[a] &lt;- S1
    s = S                                 
    a = a + 1
}

plot(t, energy, main=&quot;Energía - Tiempo&quot;, xlab=&quot;Tiempo&quot;, ylab=&quot;Energia (t)&quot;, type=&quot;l&quot;, col=&quot;blue&quot;)
lines(t, energy2)</code></pre>
<p><img src="/posts/stationarity_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>If we now put our two realizations in a three coordinate system:</p>
<pre class="r"><code>library(rgl)

rgl::plot3d(c(t,t), 
            c(energy, energy2), 
            c(rep(1, length(t)), rep(2, length(t))), 
            xlab = &#39;t&#39;, 
            ylab = &#39;E(t)&#39;, 
            zlab = &#39;S&#39;, 
            col = &#39;red&#39;)</code></pre>
<p><img src="/img/stationarity_files/3d.JPG" /></p>
So my stochastic process will be given by
<div>
<span class="math inline">\(X(S,t)\)</span>, where S represents the number of realizations and t represents time. If now I set the time, for that certain
<div>
<p><span class="math inline">\(t_{i}\)</span> I will have two energy values, one for each realization. Setting the instant of time, what happens is that a random variable appears (with all the properties of random variables). However, if I set the realization what I am left with is a function of a variable (of time in this case). Therefore, we can define a stochastic process equivalently as:</p>
<ul>
<li><p>As a set of temporary realizations and a random index that selects one of them.</p></li>
<li><p>As a set of random variables indexed by the index t.</p></li>
</ul>
</div>
<div id="what-is-stationarity" class="section level2">
<h2>What is stationarity?</h2>
<p>In the first place, it is important to highlight that there are multiple definitions of stationarity, among which we will highlight the following two:</p>
<ul>
<li><p><em>Strong stationarity:</em> A stochastic process is strong stationary if it is true that the probability that the process takes on a certain value in each state remains constant over time.</p></li>
<li><p><em>Weak stationarity:</em> This is the definition typically used in time series analysis. A stochastic process is considered weak stationary if it meets the following three properties:</p>
<ol style="list-style-type: decimal">
<li><div>
<p><span class="math inline">\(E[X_t] = \mu \ \ \forall t \in T\)</span> , that is, constant mean.</p></li>
<li><div>
<p><span class="math inline">\(E[X_t^2] &lt; \infty \ \ \forall t \in T\)</span>, that is, that the second moment is finite at any time, which guarantees that the variance is finite.</p></li>
<li><div>
<p><span class="math inline">\(Cov(X_{t1},X_{t2}) = Cov(X_{t1+h},X_{t2+h}) \ \ \forall t \in \mathbb{N}, \forall h \in \mathbb{Z}\)</span></p></li>
</ol>
<p>Property 3 inherits homoscedasticity because:</p>
<div>
<p><span class="math inline">\(Cov(X_t, X_t) = Cov(X_{t+h}, X_{t+h}) \leftrightarrow Var[X_t] = Var[X_{t+h}]\ \ \forall t \in \mathbb{N}, \forall h \in \mathbb{Z}\)</span></p>
<p>which implies that the variance of each state of the process is the same.</p></li>
</ul>
<p>To see it much more clearly, let’s look at some examples:</p>
<p><img src="/img/stationarity_files/media.JPG" /></p>
<p><img src="/img/stationarity_files/homocedasticidad.JPG" /></p>
<p><img src="/img/stationarity_files/autocovarianza.JPG" /></p>
<div id="why-is-it-important-to-know-if-a-time-series-is-stationary" class="section level3">
<h3>Why is it important to know if a time series is stationary?</h3>
<p>Mainly, for a question of predictability. Stationary series are much easier to predict. We should note here the abuse of language, <em>since the time series is not stationary, if not the stochastic process that generates it</em>. (This being clarified, we will not make a distinction from here on). But then how do I know if my time series is stationary? Here we can help ourselves in three ways:</p>
<ul>
<li><p>Visual Methods, like those seen in the previous images.</p></li>
<li><p>Local: For example, measuring the mean / variance in different periods of time and observing if it changes.</p></li>
<li><p>Statistical tests: The Augmented Dickey-Fuller test.</p></li>
</ul>
</div>
<div id="dickey-fuller-test" class="section level3">
<h3>Dickey-Fuller Test</h3>
<p>The null hypothesis of the Dickey-Fuller test is that there is a unit root (that is, the series is not stationary), while the alternative hypothesis is that there is no unit root and therefore the series is stationary. To reject the null hypothesis we will have to obtain a t-test statistic that is more negative than the critical value, as shown in the following image:</p>
<p><img src="/img/stationarity_files/dick_fuller.jpg" /></p>
<p>Let’s see an example of implementation in R:</p>
<pre class="r"><code>library(urca)
library(timetk)
library(magrittr, include.only = &#39;%&gt;%&#39;)

df &lt;- timetk::walmart_sales_weekly

df %&gt;% timetk::plot_time_series(.date_var = Date,
                                .value = Fuel_Price,
                                .smooth = F,
                                .interactive = F)</code></pre>
<p><img src="/posts/stationarity_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<pre class="r"><code>adf &lt;-urca::ur.df(df$Fuel_Price, selectlags = &#39;AIC&#39;)

urca::summary(adf)</code></pre>
<pre><code>## 
## ############################################### 
## # Augmented Dickey-Fuller Test Unit Root Test # 
## ############################################### 
## 
## Test regression none 
## 
## 
## Call:
## lm(formula = z.diff ~ z.lag.1 - 1 + z.diff.lag)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.90849 -0.03126 -0.00127  0.03665  0.23581 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## z.lag.1    -0.0003160  0.0008588  -0.368    0.713    
## z.diff.lag  0.2773029  0.0304521   9.106   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.08814 on 997 degrees of freedom
## Multiple R-squared:  0.0768, Adjusted R-squared:  0.07495 
## F-statistic: 41.47 on 2 and 997 DF,  p-value: &lt; 2.2e-16
## 
## 
## Value of test-statistic is: -0.368 
## 
## Critical values for test statistics: 
##       1pct  5pct 10pct
## tau1 -2.58 -1.95 -1.62</code></pre>
<p>We can observe how the statistic is -0.368, so we cannot reject the null hypothesis and therefore the series is not stationary. But then I can’t apply those algorithms that require my series to be stationary? Fortunately, there are methods to transform the series and make it stationary.</p>
</div>
<div id="differentiation" class="section level3">
<h3>Differentiation</h3>
The differentiation consists of taking our time series and subtracting each value from its previous value (note that in this way, we are shortening the time series in an observation). The intuition by which this transformation makes a stationary time series is the assumption that your series follows a behavior like the following:
<div>
<p><span class="math inline">\(y_{t} = \beta_{0} + \beta_{1}*t + \epsilon_{t}\)</span>.</p>
<p>The differentiation process would consist of the following:</p>
<div>
<p><span class="math inline">\(z_{t} = y_{t} - y_{t-1}\)</span></p>
<p>substituting in the previous equation we obtain:</p>
<div>
<p><span class="math inline">\((\beta_{0} + \beta_{1}*t + \epsilon_{t}) - (\beta_{0} + \beta_{1}*(t-1) + \epsilon_{t-1}) = \beta_{1} + (\epsilon_{t} - \epsilon_{t-1})\)</span></p>
As we can see, the mean of the process will be constant and equal to
<div>
<span class="math inline">\(\beta_{1}\)</span> since the epsilon terms commonly follow a normal distribution so their mean will be zero. The variance will also be constant since I will have the sum of two independent random variables (since the process that originates them is the same, then the variance will be equal to
<div>
<p><span class="math inline">\(2k^{2}\)</span>)</p>
<pre class="r"><code>df &lt;- df %&gt;% timetk::tk_augment_differences(.value = Fuel_Price)

df %&gt;% timetk::plot_time_series(.date_var = Date,
                                .value = Fuel_Price_lag1_diff1,
                                .smooth = F,
                                .interactive = F) </code></pre>
<p><img src="/posts/stationarity_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code>adf &lt;- urca::ur.df(diff(df$Fuel_Price), selectlags = &#39;AIC&#39;)

urca::summary(adf)</code></pre>
<pre><code>## 
## ############################################### 
## # Augmented Dickey-Fuller Test Unit Root Test # 
## ############################################### 
## 
## Test regression none 
## 
## 
## Call:
## lm(formula = z.diff ~ z.lag.1 - 1 + z.diff.lag)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.91057 -0.03206 -0.00014  0.03426  0.22538 
## 
## Coefficients:
##            Estimate Std. Error t value Pr(&gt;|t|)    
## z.lag.1    -0.68369    0.03806  -17.96   &lt;2e-16 ***
## z.diff.lag -0.05445    0.03165   -1.72   0.0857 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.08805 on 996 degrees of freedom
## Multiple R-squared:  0.3632, Adjusted R-squared:  0.3619 
## F-statistic: 284.1 on 2 and 996 DF,  p-value: &lt; 2.2e-16
## 
## 
## Value of test-statistic is: -17.962 
## 
## Critical values for test statistics: 
##       1pct  5pct 10pct
## tau1 -2.58 -1.95 -1.62</code></pre>
<p>As we can see, now we can reject the null hypothesis that the time series has a unit root and is non-stationary! In future posts, we will see how to model our series, if you don’t want to miss it, follow me on <a href="https://www.linkedin.com/in/alberto-gonz%C3%A1lez-almui%C3%B1a-b1176881/">LinkedIn</a>.</p>
</div>
</div>
