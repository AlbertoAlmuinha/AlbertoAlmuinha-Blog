---
title: 'Tidymodels: the `recipes` package '
author: "Alberto AlmuiÃ±a"
date: '2020-08-23T21:13:14-05:00'
description: Learn how to preprocess your data in a simple and consistent way.
slug: recipes
categories: 
- tidymodels
tags:
- preprocess
- recipes
---

****
## What is tidymodels?
****

`tidymodels` is a new framework consisting of a series of packages that facilitate the modeling process in data science projects. It allows in a unified way to perform resampling, data preprocessing, unified model interface and results validation. The complete cycle would be as follows:

![](/img/recipes_files/cycle.jpg)


In this post we will focus on the step of data preprocessing with the recipes package.

****
## Introduction to `recipes`
****

This package is born from the effort of bringing together all the steps of data preparation before applying a model in a simple, efficient and consistent way. `recipes` is born from the analogy between preparing a kitchen recipe and preprocessing your data ... what is the similarity? Both follow a few steps before cooking (modeling).

Every recipe consists of four fundamental steps:

* `recipe()`: The formula is specified (predictor variables and response variables).

* `step_xxx()`: Define the steps such missing values imputation, dummy variables, centering, scaling and so on.

* `prep()`: Preparation of the recipe. This means that a dataset is used to analyze each step on it.

* `bake()`: Apply the preprocessing steps to your datasets.


In this post we will cover these four parts and we will see examples of good practices. I hope I can convince you that from now on, tidymodels in general and recipes in particular, are your reference ecosystem.

## A brief example: Airquality dataset

```{r, message=FALSE, warning=FALSE}
library(rsample)
library(recipes)

data("airquality")

head(airquality) %>% 
  knitr::kable('html') %>% 
  kableExtra::kable_styling(position = 'left', 
                            font_size = 10, 
                            fixed_thead = list(enabled = T, background = "blue")
                            ) 
```

First, we are going to separate our dataset into a training dataset and a validation one. We will use 80% of our data to train and the remaining 20% to validate.

```{r}
(.df<-airquality %>% initial_split(prop = 0.8))

.df_train<-.df %>% training()

.df_test<-.df %>% testing()
```


You can see how `rsample` throws us a split object, where we are told how many records are used in each dataset and the total one. With the `training()` and `testing()` methods we can extract the corresponding data.

**Creating a recipe object**

Now it's time to create our first recipe.

```{r}
.recipe<-recipe(Ozone ~ ., data = .df_train)

summary(.recipe)
```

It can be seen how behind the scenes `recipes` assigns to each variable a type and a role. This will allow us to subsequently select which variables to apply a preprocessing step based on their typology or role. 
A very interesting option that allows recipes is to update the roles of the variables. For example, in this dataset we have two columns that have missing values: 'Ozone' and 'Solar.R'. We can assign these variables a specific role that will later allow us to identify them. We will create the new role 'NA_Variable' with the `udpate_role()` function:

```{r}
.recipe<-.recipe %>% update_role(Ozone, Solar.R, new_role = 'NA_Variable')

summary(.recipe)
```


**Selecting the preprocessing steps**

Once the recipe is created, it is the turn to add the necessary steps to carry out the preprocessing of the data. We have many steps to choose from:

```{r}
ls('package:recipes', pattern = '^step_')
```

Some of the most common are:

* **`step_XXXimpute():`** Methods to impute missing values such as meanimpute, medianimpute, knnimpute ...

* **`step_range():`** Normalize numeric data to be within a pre-defined range of values.

* **`step_center():`** Normalize numeric data to have a mean of zero.

* **`step_scale():`** Normalize numeric data to have a standard deviation of one.

* **`step_dummy():`** Convert nominal data (e.g. character or factors) into one or more numeric binary model terms for the levels of the original data.

* **`step_other():`** Step that will potentially pool infrequently occurring values into an "other" category.


The order in which the steps are executed is important, as you can read on the [official page of the package](https://tidymodels.github.io/recipes/articles/Ordering.html):

1. Impute
2. Individual transformations for skewness and other issues
3. Discretize (if needed and if you have no other choice)
4. Create dummy variables
5. Create interactions
6. Normalization steps (center, scale, range, etc)
7. Multivariate transformation (e.g. PCA, spatial sign, etc)


In addition, in each step we must specify which columns that step affects. There are several ways to do it, we will mention the most common:

1. Passing the variable name in the first argument

2. Selecting by the role of the variables with `all_predictors()` and `all_outcomes()` functions. As in our case we have changed the 'default' roles to 'NA_Variable', we can use the `has_role()` function to select them.

3. Selecting by the type of the variables with `all_nominal()` and `all_numerical()` functions.

4. Using dplyr selectors as `contains()`, `starts_with()` or `ends_with()` functions.


We are going to apply some of these steps to our example:

```{r}
.recipe<-.recipe %>% 
          step_meanimpute(has_role('NA_Variable'), -Solar.R) %>%
          step_knnimpute(contains('.R'), neighbors = 3) %>%
          step_center(all_numeric(), -has_role('NA_Variable')) %>%
          step_scale(all_numeric(), -has_role('NA_Variable'))

.recipe
```

It can be seen how combining all the variable selection techniques we obtain great versatility. Also comment that when the minus sign is used it means that the columns that meet this condition are excluded from the preprocessing step. It can also be seen how the recipes object now specifies all the steps that will be carried out and on which variables.

**Preparing the recipe**

The time has come to prepare the recipe on a dataset. Once prepared, we can apply this recipe on multiple datasets:

```{r}
(.recipe<-.recipe %>% prep(.df_train))
```

It is observed how the recipe is now 'trained'.

**Baking the recipe**


Now we can apply this recipe to another dataset, for example to the test data:

```{r}
.df_test<-.recipe %>% bake(.df_test)

head(.df_test) %>% 
  knitr::kable('html') %>% 
  kableExtra::kable_styling(position = 'left', 
                            font_size = 10, 
                            fixed_thead = list(enabled = T, background = "blue")
                            ) 
```


Finally we put it all together:

```{r}
.df<-recipe(Ozone ~ ., data = .df_train) %>%
   
       update_role(Ozone, Solar.R, new_role = 'NA_Variable') %>%
   
       step_meanimpute(has_role('NA_Variable'), -Solar.R) %>%
       step_knnimpute(contains('.R'), neighbors = 3) %>%
       step_center(all_numeric(), -has_role('NA_Variable')) %>%
       step_scale(all_numeric(), -has_role('NA_Variable')) %>%
   
       prep(.df_train) %>%
       bake(.df_test)
  
head(.df) %>% 
  knitr::kable('html') %>% 
  kableExtra::kable_styling(position = 'left', 
                            font_size = 10, 
                            fixed_thead = list(enabled = T, background = "blue")
                            ) 

```


As you have seen, this package offers a wide range of possibilities and facilities for carrying out the preprocessing task. Many other interesting topics about this package have been left out of this post: creating your own preprocessing step or combining this package with rsamples to apply multiple recipes to partitions used by bootstrapping or cv-Folds techniques. Maybe for a next post.


